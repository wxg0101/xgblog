---
layout: post                    # 使用的布局（不需要改）
title: "ML WEEK 1&2"              # 标题 
subtitle: 入门machine learning #副标题
date: 2022-01-07             # 时间
author: Leowxg                      # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - ML
---
# Supervised learning /Unsupervised learning

- Supervised learning

what?

Right answer given

Regression: predict continuous valued output

- classification

Discrete valued output

- Unsupervised learning

No answer given

# Linear Regression with one variable

- cost function



![image-20220107174128892](C:/Users/13936/AppData/Roaming/Typora/typora-user-images/image-20220107174128892.png)

goal: find min J(theta 1, theta 2 ...)

![image-20220107174305751](https://s2.loli.net/2022/01/07/qdAjaFgzIB3Gpev.png)

- gradient Descent

A  way to find min cost J

- Gradient descent algorithm:

<img src="https://s2.loli.net/2022/01/07/MFCojelKaOtrA3d.png" alt="image-20220107174814062" style="zoom:80%;" />

- Gradient Descent Intuition：

![image-20220107175103660](https://s2.loli.net/2022/01/07/91hGguNleWEan6H.png)

其中𝑎是学习率（learning rate），它决定了我们沿着能让代价函数下降程度最大的方向 向下迈出的步子有多大，在批量梯度下降中，我们每一次都**同时**让所有的参数减去学习速率 乘以代价函数的导数

#  Gradient Descent For Linear Regression

梯度下降算法和线性回归算法比较如图：

![image-20220107175413632](https://s2.loli.net/2022/01/07/xcRqVrNtQC1ZBF6.png)

对我们之前的线性回归问题运用梯度下降法，关键在于求出代价函数的导数，即：

<img src="https://s2.loli.net/2022/01/07/xXAk689K4rs2bMO.png" alt="image-20220107181055937" style="zoom:67%;" />

- “Batch” Gradient Descent

”批量梯度下降”，指的是在梯度下降的每一步中，我们都用到了 所有的训练样本，在梯度下降中，在计算微分求导项时，我们需要进行求和运算，所以，在 每一个单独的梯度下降中，我们最终都要计算这样一个东西，这个项需要对所有𝑚个训练样 本求和。

- Normal Equations

在数据量较大的情况下，梯 度下降法比正规方程要更适用一些。


# Review

matrix&vector 线代基础



# WEEK　２

## Linear Regression with multiple variables

- Hypothesis :

$$
h_ \theta(x)= \theta_0 + \theta_1x_1 + \theta_2x_2 + ...+ \theta_nx_n
$$

- Cost function :

  <img src="https://s2.loli.net/2022/01/07/bSYP4emwFRlAdTB.png" alt="image-20220107182341231" style="zoom: 67%;" />

### 



## Gradient Descent for Multiple Variables

### Gradient Descent in Practice I - Feature Scaling

![image-20220107182716019](https://s2.loli.net/2022/01/07/9fiArxeRgz64Mon.png)

最简单的方法是令 <img src="https://s2.loli.net/2022/01/07/fM2rQOm7qPHGDYc.png" alt="image-20220107182928592" style="zoom:80%;" />，其中 𝜇𝑛是平均值，𝑠𝑛是标准差。

### Gradient Descent in Practice II - Learning Rate

梯度下降算法收敛所需要的迭代次数根据模型的不同而不同，我们不能提前预知，我们 可以绘制迭代次数和代价函数的图表来观测算法在何时趋于收敛。

![image-20220107183013437](https://s2.loli.net/2022/01/07/7jLRtdgnQp6HiC1.png)



# Features and Polynomial Regression

# Normal Equation

# Normal Equation Noninvertibility

# Octave

