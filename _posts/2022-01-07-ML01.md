---
layout: post                    # ä½¿ç”¨çš„å¸ƒå±€ï¼ˆä¸éœ€è¦æ”¹ï¼‰
title: "ML WEEK 1&2"              # æ ‡é¢˜ 
subtitle: å…¥é—¨machine learning #å‰¯æ ‡é¢˜
date: 2022-01-07             # æ—¶é—´
author: Leowxg                      # ä½œè€…
header-img: img/post-bg-2015.jpg    #è¿™ç¯‡æ–‡ç« æ ‡é¢˜èƒŒæ™¯å›¾ç‰‡
catalog: true                       # æ˜¯å¦å½’æ¡£
tags:                               #æ ‡ç­¾
    - ML
---
# Supervised learning /Unsupervised learning

- Supervised learning

what?

Right answer given

Regression: predict continuous valued output

- classification

Discrete valued output

- Unsupervised learning

No answer given

# Linear Regression with one variable

- cost function



![image-20220107174128892](C:/Users/13936/AppData/Roaming/Typora/typora-user-images/image-20220107174128892.png)

goal: find min J(theta 1, theta 2 ...)

![image-20220107174305751](https://s2.loli.net/2022/01/07/qdAjaFgzIB3Gpev.png)

- gradient Descent

A  way to find min cost J

- Gradient descent algorithm:

<img src="https://s2.loli.net/2022/01/07/MFCojelKaOtrA3d.png" alt="image-20220107174814062" style="zoom:80%;" />

- Gradient Descent Intuitionï¼š

![image-20220107175103660](https://s2.loli.net/2022/01/07/91hGguNleWEan6H.png)

å…¶ä¸­ğ‘æ˜¯å­¦ä¹ ç‡ï¼ˆlearning rateï¼‰ï¼Œå®ƒå†³å®šäº†æˆ‘ä»¬æ²¿ç€èƒ½è®©ä»£ä»·å‡½æ•°ä¸‹é™ç¨‹åº¦æœ€å¤§çš„æ–¹å‘ å‘ä¸‹è¿ˆå‡ºçš„æ­¥å­æœ‰å¤šå¤§ï¼Œåœ¨æ‰¹é‡æ¢¯åº¦ä¸‹é™ä¸­ï¼Œæˆ‘ä»¬æ¯ä¸€æ¬¡éƒ½**åŒæ—¶**è®©æ‰€æœ‰çš„å‚æ•°å‡å»å­¦ä¹ é€Ÿç‡ ä¹˜ä»¥ä»£ä»·å‡½æ•°çš„å¯¼æ•°

#  Gradient Descent For Linear Regression

æ¢¯åº¦ä¸‹é™ç®—æ³•å’Œçº¿æ€§å›å½’ç®—æ³•æ¯”è¾ƒå¦‚å›¾ï¼š

![image-20220107175413632](https://s2.loli.net/2022/01/07/xcRqVrNtQC1ZBF6.png)

å¯¹æˆ‘ä»¬ä¹‹å‰çš„çº¿æ€§å›å½’é—®é¢˜è¿ç”¨æ¢¯åº¦ä¸‹é™æ³•ï¼Œå…³é”®åœ¨äºæ±‚å‡ºä»£ä»·å‡½æ•°çš„å¯¼æ•°ï¼Œå³ï¼š

<img src="https://s2.loli.net/2022/01/07/xXAk689K4rs2bMO.png" alt="image-20220107181055937" style="zoom:67%;" />

- â€œBatchâ€ Gradient Descent

â€æ‰¹é‡æ¢¯åº¦ä¸‹é™â€ï¼ŒæŒ‡çš„æ˜¯åœ¨æ¢¯åº¦ä¸‹é™çš„æ¯ä¸€æ­¥ä¸­ï¼Œæˆ‘ä»¬éƒ½ç”¨åˆ°äº† æ‰€æœ‰çš„è®­ç»ƒæ ·æœ¬ï¼Œåœ¨æ¢¯åº¦ä¸‹é™ä¸­ï¼Œåœ¨è®¡ç®—å¾®åˆ†æ±‚å¯¼é¡¹æ—¶ï¼Œæˆ‘ä»¬éœ€è¦è¿›è¡Œæ±‚å’Œè¿ç®—ï¼Œæ‰€ä»¥ï¼Œåœ¨ æ¯ä¸€ä¸ªå•ç‹¬çš„æ¢¯åº¦ä¸‹é™ä¸­ï¼Œæˆ‘ä»¬æœ€ç»ˆéƒ½è¦è®¡ç®—è¿™æ ·ä¸€ä¸ªä¸œè¥¿ï¼Œè¿™ä¸ªé¡¹éœ€è¦å¯¹æ‰€æœ‰ğ‘šä¸ªè®­ç»ƒæ · æœ¬æ±‚å’Œã€‚

- Normal Equations

åœ¨æ•°æ®é‡è¾ƒå¤§çš„æƒ…å†µä¸‹ï¼Œæ¢¯ åº¦ä¸‹é™æ³•æ¯”æ­£è§„æ–¹ç¨‹è¦æ›´é€‚ç”¨ä¸€äº›ã€‚


# Review

matrix&vector çº¿ä»£åŸºç¡€



# WEEKã€€ï¼’

## Linear Regression with multiple variables

- Hypothesis :

$$
h_ \theta(x)= \theta_0 + \theta_1x_1 + \theta_2x_2 + ...+ \theta_nx_n
$$

- Cost function :

  <img src="https://s2.loli.net/2022/01/07/bSYP4emwFRlAdTB.png" alt="image-20220107182341231" style="zoom: 67%;" />

### 



## Gradient Descent for Multiple Variables

### Gradient Descent in Practice I - Feature Scaling

![image-20220107182716019](https://s2.loli.net/2022/01/07/9fiArxeRgz64Mon.png)

æœ€ç®€å•çš„æ–¹æ³•æ˜¯ä»¤ <img src="https://s2.loli.net/2022/01/07/fM2rQOm7qPHGDYc.png" alt="image-20220107182928592" style="zoom:80%;" />ï¼Œå…¶ä¸­ ğœ‡ğ‘›æ˜¯å¹³å‡å€¼ï¼Œğ‘ ğ‘›æ˜¯æ ‡å‡†å·®ã€‚

### Gradient Descent in Practice II - Learning Rate

æ¢¯åº¦ä¸‹é™ç®—æ³•æ”¶æ•›æ‰€éœ€è¦çš„è¿­ä»£æ¬¡æ•°æ ¹æ®æ¨¡å‹çš„ä¸åŒè€Œä¸åŒï¼Œæˆ‘ä»¬ä¸èƒ½æå‰é¢„çŸ¥ï¼Œæˆ‘ä»¬ å¯ä»¥ç»˜åˆ¶è¿­ä»£æ¬¡æ•°å’Œä»£ä»·å‡½æ•°çš„å›¾è¡¨æ¥è§‚æµ‹ç®—æ³•åœ¨ä½•æ—¶è¶‹äºæ”¶æ•›ã€‚

![image-20220107183013437](https://s2.loli.net/2022/01/07/7jLRtdgnQp6HiC1.png)



# Features and Polynomial Regression

# Normal Equation

# Normal Equation Noninvertibility

# Octave

