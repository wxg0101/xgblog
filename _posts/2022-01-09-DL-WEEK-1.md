---
layout: post                    # 使用的布局（不需要改）
title: "DL-WEEK-1&2"              # 标题 
subtitle: Deep learning  #副标题
date: 2022-01-09             # 时间
author: Leowxg                      # 作者
header-img: img/post-bg-hacker.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - DL
---
# WEEK 1

## What is a Neural Network

## Supervised learning with neural network

对于图像应用，我们经常在神经网络上使用卷积（Convolutional Neural Network），通 常缩写为 CNN。对于序列数据，例如音频，有一个时间组件，随着时间的推移，音频被播放 出来，所以音频是最自然的表现。作为一维时间序列（两种英文说法 one-dimensional time  series / temporal sequence）.对于序列数据，经常使用 RNN，一种递归神经网络（Recurrent  Neural Network），语言，英语和汉语字母表或单词都是逐个出现的，所以语言也是最自然 的序列数据，因此更复杂的 RNNs 版本经常用于这些应用。 对于更复杂的应用比如自动驾驶，你有一张图片，可能会显示更多的 CNN 卷积神经网 络结构，其中的雷达信息是完全不同的，你可能会有一个更定制的，或者一些更复杂的混合 的神经网络结构。所以为了更具体地说明什么是标准的 CNN 和 RNN 结构，在文献中你可能 见过左图这样的图片，这是一个标准的神经网络。而右图是一个卷积神经网络的例子。

![image-20220109175027454](https://s2.loli.net/2022/01/09/tIuhAwD6Eq54apJ.png)

<img src="https://s2.loli.net/2022/01/09/t8sMXlHQVrFeK7o.png" alt="image-20220109175134621" style="zoom:67%;" />

- Structured Data

<img src="https://s2.loli.net/2022/01/09/tTOLejkU3x56ayD.png" alt="image-20220109175249313" style="zoom:67%;" />

- Unstructured Data 

<img src="https://s2.loli.net/2022/01/09/hPUjNeOWHCotxyQ.png" alt="image-20220109175340990" style="zoom:67%;" />



## Why is Deep Learning taking off

Scale drives deep learning progress

- data
- computation
- algorithms

# WEEK 2

## Basic of Neural Network Programming

### Binary Classification

> 符号定义：
> $$
> x:表示一个n_x维数据，为输入数据，维度为（n_x, 1)
> $$
>
> $$
> y:表示输出结果，取值为（0，1）
> $$
>
> $(x^i,y^i)$:表示第i组数据
>
> 𝑋 = [𝑥 (1) , 𝑥 (2) , . . . , 𝑥 (𝑚) ]：表示所有的训练数据集的输入值，放在一个 𝑛𝑥 × 𝑚的矩阵中， 其中𝑚表示样本数目;  𝑌 = [𝑦 (1) , 𝑦 (2) , . . . , 𝑦 (𝑚) ]：对应表示所有训练数据集的输出值，维度为1 × 𝑚。 用一对(𝑥, 𝑦)来表示一个单独的样本，𝑥代表𝑛𝑥维的特征向量，𝑦 表示标签(输出结果)只能为 0 或 1。

### Logistic regression

- sigmoid函数，如图($\sigma$)

![image-20220109192741528](https://s2.loli.net/2022/01/09/ZmohcYNUTi2aEPx.png)

我们用𝑤来表示逻辑回归的参 数，这也是一个𝑛𝑥维向量（因为𝑤实际上是特征权重，维度与特征向量相同），参数里面还 有𝑏，这是一个实数（表示偏差）。所以给出输入𝑥以及参数𝑤和𝑏之后，我们怎样产生输出 预测值 $\hat{y}$，你可以尝试却不可行的事是让 $\hat{y}$ = $𝑤^𝑇𝑥$ + 𝑏。*但是y应该在0到1之间*

所以我们的输出应该是$\hat{y}$等于由上面得到的线性函数式子作为自变量的 sigmoid 函数中，公式如上图最下面所示，将线性函数转换为非线性函数。



### Logistic regression cost function

> to train the parameters W and B of the logistic regression model 

逻辑回归的输出函数

![image-20220109194039431](https://s2.loli.net/2022/01/09/IHJSAWCNf9Kl14U.png)

- Loss function:

> to measure how good our output y-hat is when the true label is y

- In this model we use :

![image-20220109194505739](https://s2.loli.net/2022/01/09/ylmkrXbEfnV3LOB.png)

- why use this loss function

 当𝑦 = 1时损失函数𝐿 = −log($\hat{y}$)，如果想要损失函数𝐿尽可能得小，那么$\hat{y}$就要尽可能大， 因为 sigmoid 函数取值[0,1]，所以𝑦^会无限接近于 1

当𝑦 = 0时损失函数𝐿 = −log(1 − $\hat{y}$)，如果想要损失函数𝐿尽可能得小，那么$\hat{y}$就要尽可 能小，因为 sigmoid 函数取值[0,1]，所以𝑦^会无限接近于 0。

**it was defined with respect to a single training example**

- cost function

> it measures how well you are doing an **entire** training set

![image-20220109195047015](https://s2.loli.net/2022/01/09/2SmqHpg73uYoPbM.png)



### Gradient Descent

> to find w,b that minimize J(w,b)

repeat:

![image-20220109200028614](https://s2.loli.net/2022/01/09/DWRq8L5OQZgonUr.png)

逻辑回归的代价函数（成本函数）𝐽(𝑤, 𝑏)是含有两个参数的。

![image-20220109200612322](https://s2.loli.net/2022/01/09/9TQHXqbBrEcJmeR.png)

### Deriatives

> 略

### Computation Graph

> 神经网络的计算，都是按照前向或反向传播过程组织的。首先我们计算出 一个新的网络的输出（前向过程），紧接着进行一个反向传输操作。后者我们用来计算出对应的梯度或导数。计算图解释了为什么我们用这种方式组织这些计算过程。

![image-20220109201605073](https://s2.loli.net/2022/01/09/hTsSAwxCRlOYXVB.png)

**计算图组织计算的形式是用蓝色箭头从左到右的计算**

### Derivatives with a Computation  Graph

> 略

### Logistic Regression Gradient  Descent 

![image-20220109203932831](https://s2.loli.net/2022/01/09/Y4neKIQuDTlhSBR.png)



反向计算出代价函 数𝐿(𝑎, 𝑦)关于𝑎的导数，在编写代码时，只需要用𝑑𝑎 来表示

![image-20220109204024772](https://s2.loli.net/2022/01/09/OfmplMIn7j9W41y.png)

![image-20220109204041843](https://s2.loli.net/2022/01/09/SLbRnZQNidhKs9I.png)

![image-20220109204121395](https://s2.loli.net/2022/01/09/bm7WLuiZ5wJ6fCV.png)

![image-20220109204142895](https://s2.loli.net/2022/01/09/h8jfLWiVFrAenqu.png)

现在进行最后一步反向推导，也就是计算𝑤和𝑏变化对代价函数𝐿的影响，特别地，可以用:

![image-20220109204227495](https://s2.loli.net/2022/01/09/2qMYANld5rujJkn.png)

![image-20220109204244847](https://s2.loli.net/2022/01/09/kO5mwuSjerQLI1W.png)

### Gradient Descent on m Examples

![image-20220109210315780](https://s2.loli.net/2022/01/09/QcH37uFsb6oDnkW.png)

现在代码中显式地使用 for 循环使你的算法很低效，同 时在深度学习领域会有越来越大的数据集。所以能够应用你的算法且没有显式的 for 循环会 是重要的，并且会帮助你适用于更大的数据集。所以这里有一些叫做向量化技术,它可以允 许你的代码摆脱这些显式的 for 循环

### Vectorization&More Examples of Vectorization

> 同machine learning

### Vectorizing Logistic Regression

> 向量化方程式

![image-20220109214759099](https://s2.loli.net/2022/01/09/oWDtzCrqUXMGhiu.png)

- numpy 命令是𝑍 = 𝑛𝑝. 𝑑𝑜𝑡(𝑤. 𝑇,𝑋) + 𝑏。 Python 中𝑏 是一个实数，只是一个普通的实数。但是 当你将这个向量加上这个实数时，Python 自动把这个实数 𝑏 扩展成一个 1 × 𝑚 的行向量。 所以这种情况下的操作似乎有点不可思议，它在 Python 中被称作广播(brosdcasting)

- 这一行代码：𝐴 = [𝑎 (1) 𝑎 (2) . . .  𝑎 (𝑚) ] = 𝜎(𝑍) ，通过恰当地运用𝜎一次性计算所有$\alpha$



### Vectorizing Logistic  Regression's Gradient

> 向量化计算公式：
>
> 

![image-20220109222815538](https://s2.loli.net/2022/01/09/NXa3vJ7C8PQGdRc.png)

### Broadcasting in Python

![image-20220109224407901](https://s2.loli.net/2022/01/09/dVfWTxo8EBKLpHI.png)

![image-20220109224422388](https://s2.loli.net/2022/01/09/GDpvH5ZWbaARhw4.png)

其中 sum 的参数 axis=0 表示求和运算按列执行

axis 用来指明将要进行的运算 是沿着哪个轴执行，在 numpy 中，0 轴是垂直的，也就是列，而 1 轴是水平的，也就是行。 而第二个 A/cal.reshape(1,4)指令则调用了 numpy 中的广播机制。这里使用 3 × 4 的矩阵𝐴除以 1 × 4的矩阵𝑐𝑎𝑙。技术上来讲，其实并不需要再将矩阵𝑐𝑎𝑙 reshape(重塑)成 1 × 4，因为矩阵𝑐𝑎𝑙本身已经是 1 × 4了。但是当我们写代码时不确定矩阵维度的时候，通 常会对矩阵进行重塑来确保得到我们想要的列向量或行向量。重塑操作 reshape 是一个常 量时间的操作，时间复杂度是𝑂(1)，它的调用代价极低。

=======
---
layout: post                    # 使用的布局（不需要改）
title: "DL-WEEK-1&2"              # 标题 
subtitle: Deep learning  #副标题
date: 2022-01-09             # 时间
author: Leowxg                      # 作者
header-img: img/post-bg-hacker.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - DL
---
# WEEK 1

## What is a Neural Network

## Supervised learning with neural network

对于图像应用，我们经常在神经网络上使用卷积（Convolutional Neural Network），通 常缩写为 CNN。对于序列数据，例如音频，有一个时间组件，随着时间的推移，音频被播放 出来，所以音频是最自然的表现。作为一维时间序列（两种英文说法 one-dimensional time  series / temporal sequence）.对于序列数据，经常使用 RNN，一种递归神经网络（Recurrent  Neural Network），语言，英语和汉语字母表或单词都是逐个出现的，所以语言也是最自然 的序列数据，因此更复杂的 RNNs 版本经常用于这些应用。 对于更复杂的应用比如自动驾驶，你有一张图片，可能会显示更多的 CNN 卷积神经网 络结构，其中的雷达信息是完全不同的，你可能会有一个更定制的，或者一些更复杂的混合 的神经网络结构。所以为了更具体地说明什么是标准的 CNN 和 RNN 结构，在文献中你可能 见过左图这样的图片，这是一个标准的神经网络。而右图是一个卷积神经网络的例子。

![image-20220109175027454](https://s2.loli.net/2022/01/09/tIuhAwD6Eq54apJ.png)

<img src="https://s2.loli.net/2022/01/09/t8sMXlHQVrFeK7o.png" alt="image-20220109175134621" style="zoom:67%;" />

- Structured Data

<img src="https://s2.loli.net/2022/01/09/tTOLejkU3x56ayD.png" alt="image-20220109175249313" style="zoom:67%;" />

- Unstructured Data 

<img src="https://s2.loli.net/2022/01/09/hPUjNeOWHCotxyQ.png" alt="image-20220109175340990" style="zoom:67%;" />



## Why is Deep Learning taking off

Scale drives deep learning progress

- data
- computation
- algorithms

# WEEK 2

## Basic of Neural Network Programming

### Binary Classification

> 符号定义：
> $$
> x:表示一个n_x维数据，为输入数据，维度为（n_x, 1)
> $$
>
> $$
> y:表示输出结果，取值为（0，1）
> $$
>
> $(x^i,y^i)$:表示第i组数据
>
> 𝑋 = [𝑥 (1) , 𝑥 (2) , . . . , 𝑥 (𝑚) ]：表示所有的训练数据集的输入值，放在一个 𝑛𝑥 × 𝑚的矩阵中， 其中𝑚表示样本数目;  𝑌 = [𝑦 (1) , 𝑦 (2) , . . . , 𝑦 (𝑚) ]：对应表示所有训练数据集的输出值，维度为1 × 𝑚。 用一对(𝑥, 𝑦)来表示一个单独的样本，𝑥代表𝑛𝑥维的特征向量，𝑦 表示标签(输出结果)只能为 0 或 1。

### Logistic regression

- sigmoid函数，如图($\sigma$)

![image-20220109192741528](https://s2.loli.net/2022/01/09/ZmohcYNUTi2aEPx.png)

我们用𝑤来表示逻辑回归的参 数，这也是一个𝑛𝑥维向量（因为𝑤实际上是特征权重，维度与特征向量相同），参数里面还 有𝑏，这是一个实数（表示偏差）。所以给出输入𝑥以及参数𝑤和𝑏之后，我们怎样产生输出 预测值 $\hat{y}$，你可以尝试却不可行的事是让 $\hat{y}$ = $𝑤^𝑇𝑥$ + 𝑏。*但是y应该在0到1之间*

所以我们的输出应该是$\hat{y}$等于由上面得到的线性函数式子作为自变量的 sigmoid 函数中，公式如上图最下面所示，将线性函数转换为非线性函数。



### Logistic regression cost function

> to train the parameters W and B of the logistic regression model 

逻辑回归的输出函数

![image-20220109194039431](https://s2.loli.net/2022/01/09/IHJSAWCNf9Kl14U.png)

- Loss function:

> to measure how good our output y-hat is when the true label is y

- In this model we use :

![image-20220109194505739](https://s2.loli.net/2022/01/09/ylmkrXbEfnV3LOB.png)

- why use this loss function

 当𝑦 = 1时损失函数𝐿 = −log($\hat{y}$)，如果想要损失函数𝐿尽可能得小，那么$\hat{y}$就要尽可能大， 因为 sigmoid 函数取值[0,1]，所以𝑦^会无限接近于 1

当𝑦 = 0时损失函数𝐿 = −log(1 − $\hat{y}$)，如果想要损失函数𝐿尽可能得小，那么$\hat{y}$就要尽可 能小，因为 sigmoid 函数取值[0,1]，所以𝑦^会无限接近于 0。

**it was defined with respect to a single training example**

- cost function

> it measures how well you are doing an **entire** training set

![image-20220109195047015](https://s2.loli.net/2022/01/09/2SmqHpg73uYoPbM.png)



### Gradient Descent

> to find w,b that minimize J(w,b)

repeat:

![image-20220109200028614](https://s2.loli.net/2022/01/09/DWRq8L5OQZgonUr.png)

逻辑回归的代价函数（成本函数）𝐽(𝑤, 𝑏)是含有两个参数的。

![image-20220109200612322](https://s2.loli.net/2022/01/09/9TQHXqbBrEcJmeR.png)

### Deriatives

> 略

### Computation Graph

> 神经网络的计算，都是按照前向或反向传播过程组织的。首先我们计算出 一个新的网络的输出（前向过程），紧接着进行一个反向传输操作。后者我们用来计算出对应的梯度或导数。计算图解释了为什么我们用这种方式组织这些计算过程。

![image-20220109201605073](https://s2.loli.net/2022/01/09/hTsSAwxCRlOYXVB.png)

**计算图组织计算的形式是用蓝色箭头从左到右的计算**

### Derivatives with a Computation  Graph

> 略

### Logistic Regression Gradient  Descent 

![image-20220109203932831](https://s2.loli.net/2022/01/09/Y4neKIQuDTlhSBR.png)



反向计算出代价函 数𝐿(𝑎, 𝑦)关于𝑎的导数，在编写代码时，只需要用𝑑𝑎 来表示

![image-20220109204024772](https://s2.loli.net/2022/01/09/OfmplMIn7j9W41y.png)

![image-20220109204041843](https://s2.loli.net/2022/01/09/SLbRnZQNidhKs9I.png)

![image-20220109204121395](https://s2.loli.net/2022/01/09/bm7WLuiZ5wJ6fCV.png)

![image-20220109204142895](https://s2.loli.net/2022/01/09/h8jfLWiVFrAenqu.png)

现在进行最后一步反向推导，也就是计算𝑤和𝑏变化对代价函数𝐿的影响，特别地，可以用:

![image-20220109204227495](https://s2.loli.net/2022/01/09/2qMYANld5rujJkn.png)

![image-20220109204244847](https://s2.loli.net/2022/01/09/kO5mwuSjerQLI1W.png)

### Gradient Descent on m Examples

![image-20220109210315780](https://s2.loli.net/2022/01/09/QcH37uFsb6oDnkW.png)

现在代码中显式地使用 for 循环使你的算法很低效，同 时在深度学习领域会有越来越大的数据集。所以能够应用你的算法且没有显式的 for 循环会 是重要的，并且会帮助你适用于更大的数据集。所以这里有一些叫做向量化技术,它可以允 许你的代码摆脱这些显式的 for 循环

### Vectorization&More Examples of Vectorization

> 同machine learning

### Vectorizing Logistic Regression

> 向量化方程式

![image-20220109214759099](https://s2.loli.net/2022/01/09/oWDtzCrqUXMGhiu.png)

- numpy 命令是𝑍 = 𝑛𝑝. 𝑑𝑜𝑡(𝑤. 𝑇,𝑋) + 𝑏。 Python 中𝑏 是一个实数，只是一个普通的实数。但是 当你将这个向量加上这个实数时，Python 自动把这个实数 𝑏 扩展成一个 1 × 𝑚 的行向量。 所以这种情况下的操作似乎有点不可思议，它在 Python 中被称作广播(brosdcasting)

- 这一行代码：𝐴 = [𝑎 (1) 𝑎 (2) . . .  𝑎 (𝑚) ] = 𝜎(𝑍) ，通过恰当地运用𝜎一次性计算所有$\alpha$



### Vectorizing Logistic  Regression's Gradient

> 向量化计算公式：
>
> 

![image-20220109222815538](https://s2.loli.net/2022/01/09/NXa3vJ7C8PQGdRc.png)

### Broadcasting in Python

![image-20220109224407901](https://s2.loli.net/2022/01/09/dVfWTxo8EBKLpHI.png)

![image-20220109224422388](https://s2.loli.net/2022/01/09/GDpvH5ZWbaARhw4.png)

其中 sum 的参数 axis=0 表示求和运算按列执行

axis 用来指明将要进行的运算 是沿着哪个轴执行，在 numpy 中，0 轴是垂直的，也就是列，而 1 轴是水平的，也就是行。 而第二个 A/cal.reshape(1,4)指令则调用了 numpy 中的广播机制。这里使用 3 × 4 的矩阵𝐴除以 1 × 4的矩阵𝑐𝑎𝑙。技术上来讲，其实并不需要再将矩阵𝑐𝑎𝑙 reshape(重塑)成 1 × 4，因为矩阵𝑐𝑎𝑙本身已经是 1 × 4了。但是当我们写代码时不确定矩阵维度的时候，通 常会对矩阵进行重塑来确保得到我们想要的列向量或行向量。重塑操作 reshape 是一个常 量时间的操作，时间复杂度是𝑂(1)，它的调用代价极低。

>>>>>>> 7c124f682f1f4d3144fe86da9cd0f996d7ccc20f
**numpy 广播机制 如果两个数组的后缘维度的轴长度相符或其中一方的轴长度为 1，则认为它们是广播兼 容的。广播会在缺失维度和轴长度为 1 的维度上进行。**