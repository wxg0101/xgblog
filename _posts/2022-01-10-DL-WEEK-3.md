---
layout: post                    # 使用的布局（不需要改）
title: "DL-WEEK-3"              # 标题 
subtitle: Deep learning #副标题
date: 2022-01-10             # 时间
author: Leowxg                      # 作者
header-img: img/post-bg-hacker.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - DL
---

# WEEK 3

## Shallow neural networks

## Neural Network Overview

![image-20220110183522967](https://s2.loli.net/2022/01/10/dWRexNpOvHrGKbT.png)

> [𝑚]表示第𝑚层网络中节点相关的数，这些节点的集合被称为第𝑚层网络

这个神经网络对应的 3 个节点，首先计算第一层网络中的各个节点相关 的数𝑧 [1]，接着计算𝛼 [1]，在计算下一层网络同理



输入特征𝑥，参数𝑤和*b*

![image-20220110183555188](https://s2.loli.net/2022/01/10/jmznHqG9XKdQfos.png)

接下来使用𝑧就可以计算出𝑎。将符号换为表示输出$\hat{y}$⟹ 𝑎 = 𝜎(𝑧),然后可以计算 出 loss function 𝐿(𝑎, 𝑦)

## Neural Network Representation

> 本例中的神经网络只包含一个隐藏层

![image-20220110184709197](https://s2.loli.net/2022/01/10/sKMonFQbRhrTdfL.png)

输入特征𝑥1、𝑥2、𝑥3，它们被竖直地堆叠起来，这叫做神经网络的输入层,然后这里有另外一层我们称之为隐藏层

在本例中最后一层只由一个结点构成，而这个只 有一个结点的层被称为输出层，它负责产生预测值

- 隐藏层：

在一个神经网络中， 用监督学习训练时，训练集包含了输入𝑥也包含了目标输出𝑦，所以术语隐藏层的含义是在训练集中，这些中间结点的准确值我们是不知道到的，也就是说你看不见它们在训练集中应具有的值。你能看见输入的值，你也能看见输出的值，但是隐藏层中的东西，在 训练集中你是无法看到的。所以这也解释了词语隐藏层，只是表示你无法在训练集中看到他们

> $𝑎^{[0]}$可以用来表示输入特征

𝑎表示激活的意思，它意味着网络中不同层的值会传递到它们 后面的层中，输入层将𝑥传递给隐藏层，所以我们将输入层的激活值称为$𝑎^{[0]}$

下一层即隐藏 层也同样会产生一些激活值,每个单元节点记为$a_1^{[1]}$,

$a_2^{[1]}$,$a_3^{[1]}$,$a_4^{[1]}$

如果写成 Python 代码，那么它是一个规模为 4x1 的矩阵或一个大小为 4 的列向量,因为在本例中，我们有四个结点或者单元，或者称为四个隐藏层单元

<img src="https://s2.loli.net/2022/01/10/vMg3jGuXrDpyVPs.png" alt="image-20220110210336487" style="zoom:67%;" />

最后输出层将产生某个数值𝑎，它只是一个单独的实数，所以的$\hat{y}$值将取为$a^{[2]}$。

**在逻辑回归中我们只有一个输出层，所以没有用带方括号的上标**

> 在约定俗成的符号传统中，在这里你所看到的这个例子， 只能叫做一个两层的神经网络
>
> 原因是当我们计算网络的层数时，输入层是不算 入总层数内，所以隐藏层是第一层，输出层是第二层。
>
> 第二个惯例是我们将输入层称为第零层，所以在技术上，这仍然是一个三层的神经网络，因为这里有输入层、隐藏层，还有输出层。但是在传统的符号使用中，如果阅读研究论文或者在这门课中，你会看到人们将这个神经网络称为一个两层的神经网络，因为我们不将输入层看作一个标准的层。

![image-20220110211206448](https://s2.loli.net/2022/01/10/Grm3es4ghH6tjc1.png)



> 我们要看到的隐藏层以及最后的输出层是带有参数的，这里的隐藏层将拥有两个 参数𝑊和𝑏，我将给它们加上上标 [1] (𝑊[1] ,𝑏 [1] )，表示这些参数是和第一层这个隐藏层有关 系的

## Computing a Neural Network's  output

> review:𝑥表示输入特征，𝑎表示每个神经元的输出，𝑊表示特征的权重，上标表示神经网 络的层数（隐藏层为 1），下标表示该层的第几个神经元

![image-20220110230604888](https://s2.loli.net/2022/01/10/1myVQJasdK3pt5A.png)

- 神经网络的计算

<img src="https://s2.loli.net/2022/01/10/57CLxZuAsDrJYbi.png" alt="image-20220110230728028" style="zoom:67%;" />

用圆圈表示神经网络的计算单元，逻辑回归的计算有两个步骤，首先按步骤计算出𝑧，然后在第二 步中以 sigmoid 函数为激活函数计算𝑧（得出𝑎），一个神经网络只是这样子做了多次重复计算。

隐藏层的第二个以及后面两个神经元的计算过程一样，只是注意符号表示不同，最终分别得到$a_1^{[1]}$,$a_2^{[1]}$,$a_3^{[1]}$,$a_4^{[1]}$，详细结果见下:<img src="https://s2.loli.net/2022/01/10/wYK6GzuXCnpTc73.png" alt="image-20220110230947807" style="zoom:67%;" />

- 向量化计算

对于神经网络的第一层，给予一个输入𝑥，得到𝑎 [1]，𝑥可以表示为𝑎 [0]。通过相似的衍生 你会发现，后一层的表示同样可以写成类似的形式，得到$𝑎^{[2]}$，$\hat{y}$ = $𝑎^{[2]}$

![image-20220110231107024](https://s2.loli.net/2022/01/10/gtayNZKLuSTGXBA.png)

![image-20220110231115949](https://s2.loli.net/2022/01/10/7CTmswfIUyxn5ga.png)

![image-20220110231309037](https://s2.loli.net/2022/01/10/ZhdcvlDIjQM75gE.png)

> 如上图左半部分所示为神经网络，把网络左边部分盖住先忽略，那么最后的输出单元就 相当于一个逻辑回归的计算单元。当你有一个包含一层隐藏层的神经网络，你需要去实现以 计算得到输出的是右边的四个等式，并且可以看成是一个向量化的计算过程，计算出隐藏层 的四个逻辑回归单元和整个隐藏层的输出结果，如果编程实现需要的也只是这四行代码。

## Vectorizing across multiple examples

> 逻辑回归是将各个训练样本组合成矩阵，对矩阵的各列进行计算。神经网络是通过对逻 辑回归中的等式简单的变形，让神经网络计算出输出值。这种计算是所有的训练样本同时进 行的，以下是实现它具体的步骤：

![image-20220110231800459](https://s2.loli.net/2022/01/10/Tr3bkdMeXFBEGRH.png)

定义矩阵𝑋等于训练样本，将它 们组合成矩阵的各列，形成一个𝑛维或𝑛乘以𝑚维矩阵

![image-20220110231906211](https://s2.loli.net/2022/01/10/prbW3TSngdVlMw2.png)

以此类推，从小写的向量𝑥到这个大写的矩阵𝑋，只是通过组合𝑥向量在矩阵的各列中。 同理，$𝑧 ^{[1](1)}$，$𝑧 ^{[1](2)}$等等都是$𝑧 ^{[1](𝑚)}$的列向量，将所有𝑚都组合在各列中，就的到矩阵 𝑍 [1]。 同理，$𝑎^{[1](1)}$，，……，$a ^{[1](2)}$将其组合在$a ^{[1](m)}$矩阵各列中，如同从向量𝑥到矩阵𝑋，以 及从向量𝑧到矩阵𝑍一样，就能得到矩阵𝐴 [1]。 同样的，对于𝑍 [2]和𝐴 [2]，也是这样得到。

## Activation functions

> 在神经网路的前向传播中，𝑎 [1] = 𝜎(𝑧 [1] )和𝑎 [2] = 𝜎(𝑧 [2] )这两步会使用到 sigmoid 函数。 sigmoid 函数在这里被称为激活函数。
>
> ![image-20220111175411910](https://s2.loli.net/2022/01/11/H1kERLrdO3pXzUF.png)

更通常的情况下，使用不同的函数𝑔(𝑧 [1] )，𝑔可以是除了 sigmoid 函数意外的非线性函 数。tanh 函数或者双曲正切函数是总体上都优于 sigmoid 函数的激活函数。

![image-20220111175421769](https://s2.loli.net/2022/01/11/SnVbByz543tEaPH.png)

在讨论优化算法时，有一点要说明：我基本已经不用 sigmoid 激活函数了，tanh 函数在 所有场合都优于 sigmoid 函数。 但有一个例外：在二分类的问题中，对于输出层，因为𝑦的值是 0 或 1，所以想让$\hat{y}$的数 值介于 0 和 1 之间，而不是在-1 和+1 之间。所以需要使用 sigmoid 激活函数。

- 修正线性单元的函数（ReLu）

![image-20220111175633089](https://s2.loli.net/2022/01/11/rg3QpS6PDaOq54t.png)

只要𝑧是正值的情况下，导数恒等于 1，当𝑧是负 值的时候，导数恒等于 0。从实际上来说，当使用𝑧的导数时，𝑧=0 的导数是没有定义的。但 是当编程实现的时候，𝑧的取值刚好等于 0.00000001，这个值相当小，所以，在实践中，不 需要担心这个值，𝑧是等于 0 的时候，假设一个导数是 1 或者 0 效果都可以。

选择激活函数的经验法则： 如果输出是 0、1 值（二分类问题），则输出层选择 sigmoid 函数，然后其它的所有单 元都选择 Relu 函数。 这是很多激活函数的默认选择，如果在隐藏层上不确定使用哪个激活函数，那么通常会 使用 Relu 激活函数。有时，也会使用 tanh 激活函数，但 Relu 的一个优点是：当𝑧是负值的 时候，导数等于 0。 这里也有另一个版本的 Relu 被称为 Leaky Relu。 当𝑧是负值时，这个函数的值不是等于 0，而是轻微的倾斜，如图。 这个函数通常比 Relu 激活函数效果要好，尽管在实际中 Leaky ReLu 使用的并不多。

**sigmoid 激活函数：除了输出层是一个二分类问题基本不会用它。 tanh 激活函数：tanh 是非常优秀的，几乎适合所有场合。 ReLu 激活函数：最常用的默认函数，，如果不确定用哪个激活函数，就使用 ReLu 或者 Leaky ReLu。**

## why need a nonlinear  activation function?

## Derivatives of activation functions

- sigmoid activation function

![image-20220111180126972](https://s2.loli.net/2022/01/11/6SU42DQGpRicfvu.png)

![image-20220111180149073](https://s2.loli.net/2022/01/11/uwQIzbAUhk8d734.png)



- Tanh activation function

![image-20220111180659245](https://s2.loli.net/2022/01/11/PqhHLinkMscEUe7.png)

![image-20220111180714541](https://s2.loli.net/2022/01/11/oXKYDOGHCRriEn7.png)

![image-20220111180735761](https://s2.loli.net/2022/01/11/Rx9ZjKs5zn46ToJ.png)



- Rectified Linear Unit (ReLU)

## Gradient descent for neural  networks

## Random+Initialization