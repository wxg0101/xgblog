---
layout: post                    # 使用的布局（不需要改）
title: "DL-WEEK-4"              # 标题 
subtitle: Deep learning #副标题
date: 2022-01-11             # 时间
author: Leowxg                      # 作者
header-img: img/post-bg-hacker.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - DL
---

# Deep Neural Networks

## Deep L-layer neural network

> ![image-20220111221311565](https://s2.loli.net/2022/01/11/GWsLaD9ObZTEt8q.png)
>
> 上图是一个四层的神经网络，有三个隐藏层。我们可以看到，第一层（即左边数过去第 二层，因为输入层是第 0 层）有 5 个神经元数目，第二层 5 个，第三层 3 个。 我们用 L 表示层数，上图：𝐿 = 4，输入层的索引为“0”，第一个隐藏层𝑛 [1] = 5,表示有 5 个隐藏神经元，同理$𝑛^{[2]} = 5，𝑛^{[3]} = 3$，$𝑛 ^{[4]}=𝑛 ^{[L]} = 1$（输出单元为 1）。而输入层，$𝑛 ^{[0]} = 𝑛_𝑥 = 3$。

## Forward and backward propagation

![image-20220111223300812](https://s2.loli.net/2022/01/11/qwP7C3JajX45huI.png)

所以前向传播的步骤可以写成：

![image-20220111223323533](https://s2.loli.net/2022/01/11/hGiXkf8dI3AvoS1.png)

![image-20220111223336953](https://s2.loli.net/2022/01/11/eg9vDK32OHBE6uw.png)

![image-20220111223400382](https://s2.loli.net/2022/01/11/aD32bVxO8Y5hjcs.png)

所以反向传播的步骤可以写成：

<img src="https://s2.loli.net/2022/01/11/9xyQRwgDsvMG2tl.png" alt="image-20220111223423742" style="zoom:80%;" />

向量化实现过程可以写成：

![image-20220111223446010](https://s2.loli.net/2022/01/11/I5tuvl3yONjUTiK.png)

## Forward propagation in a Deep  Network

![image-20220111224454775](https://s2.loli.net/2022/01/11/sG4a6Scq3xyudLz.png)

向量化实现过程可以写成：

![image-20220111224508667](https://s2.loli.net/2022/01/11/zSU5ulNLnQGtIE3.png)

## Getting your matrix dimensions right

𝑤的维度是（下一层的维数，前一层的维数），即𝑤[𝑙] : (𝑛 [𝑙] ,𝑛 [𝑙−1] )； 𝑏的维度是（下一层的维数，1），即: 𝑏 [𝑙] : (𝑛 [𝑙] , 1)； 𝑧 [𝑙] ,𝑎 [𝑙] : (𝑛 [𝑙] , 1); 𝑑𝑤[𝑙]和𝑤[𝑙]维度相同，𝑑𝑏[𝑙]和𝑏 [𝑙]维度相同，且𝑤和𝑏向量化维度不变，但𝑧,𝑎以及𝑥的维 度会向量化后发生变化。

![image-20220111230031982](https://s2.loli.net/2022/01/11/TsRbkLDBzV6GIOa.png)

向量化后： 𝑍 [𝑙]可以看成由每一个单独的𝑍 [𝑙]叠加而得到，𝑍 [𝑙] = (𝑧 [𝑙][1]，𝑧 [𝑙][2]，𝑧 [𝑙][3]，…，𝑧 [𝑙][𝑚] )， 𝑚为训练集大小，所以𝑍 [𝑙]的维度不再是(𝑛 [𝑙] , 1)，而是(𝑛 [𝑙] , 𝑚)。 𝐴 [𝑙]：(𝑛 [𝑙] , 𝑚)，𝐴 [0] = 𝑋 = (𝑛 [𝑙] , 𝑚)

![image-20220111230123894](https://s2.loli.net/2022/01/11/pxZscWFyPC7aKRt.png)

做深度神经网络的反向传播时，一定要确认所有的矩阵维数是前后一致的，可以大 大提高代码通过率。

## Why deep representations?

![image-20220111230221869](https://s2.loli.net/2022/01/11/DbA5l6cRUCrWSoT.png)

如果你在建一个人脸识别或是人脸检测系统，深度神经网络所做的事就是，当你输入一张脸部的照片，然后你可以把深度神经网络的第一层，当成一个特征探测器或者边缘探测器。在这个例子里，我会建一个大概有 20 个隐藏单元的深 度神经网络。隐藏单元就是这些图里这些小方块（第一张大图）， 举个例子，这个小方块（第一行第一列）就是一个隐藏单元，它会去找这张照片里“|”边缘的 方向。那么这个隐藏单元（第四行第四列），可能是在找（“—”）水平向的边缘在哪里。之 后的课程里，我们会讲专门做这种识别的卷积神经网络，到时候会细讲，为什么小单元是这 么表示的。你可以先把神经网络的第一层当作看图，然后去找这张照片的各个边缘。我们可 以把照片里组成边缘的像素们放在一起看，然后它可以把被探测到的边缘组合成面部的不同部分（第二张大图）。比如说，可能有一个神经元会去找眼睛的部分，另外还有别的在找鼻 子的部分，然后把这许多的边缘结合在一起，就可以开始检测人脸的不同部分。最后再把这 些部分放在一起，比如鼻子眼睛下巴，就可以识别或是探测不同的人脸（第三张大图）。

**边缘探测器其实相对来 说都是针对照片中非常小块的面积。就像这块（第一行第一列），都是很小的区域。面部探 测器就会针对于大一些的区域，但是主要的概念是，一般你会从比较小的细节入手，比如边 缘，然后再一步步到更大更复杂的区域，比如一只眼睛或是一个鼻子，再把眼睛鼻子装一块 组成更复杂的部分。**

![image-20220111230536966](https://s2.loli.net/2022/01/11/v4ADCb1eQxdE3lu.png)

深度神经网络的这许多隐藏层中，较早的前几层能学习一些低层次的简单特征，等 到后几层，就能把简单的特征结合起来，去探测更加复杂的东西

**深层的网络隐藏单元数量相对较少，隐藏层数目较多，如果浅层的网络想要达到同样的 计算结果则需要指数级增长的单元数量才能达到。**

![image-20220111230639009](https://s2.loli.net/2022/01/11/DxcKrh5NgZq6UmS.png)

## Building blocks of deep neural networks

![image-20220111230753183](https://s2.loli.net/2022/01/11/vg1wEzJy3iKIodG.png)



![image-20220111230938848](https://s2.loli.net/2022/01/11/yUFRPLgW5wDAp8f.png)

## Parameters vs Hyperparameters

- 什么是超参数？

> 比如算法中的 learning rate 𝑎（学习率）、iterations(梯度下降法循环的数量)、𝐿（隐藏 层数目）、𝑛 [𝑙]（隐藏层单元数目）、choice of activation function（激活函数的选择）都需要 你来设置，这些数字实际上控制了最后的参数𝑊和𝑏的值，所以它们被称作超参数。

- 如何寻找超参数的最优值？

> ![image-20220111231105890](https://s2.loli.net/2022/01/11/PE1uLFtCMWTNygq.png)
>
> 走 Idea—Code—Experiment—Idea 这个循环，尝试各种不同的参数，实现模型并观察是 否成功，然后再迭代。

## What does this have to do with  the brain?