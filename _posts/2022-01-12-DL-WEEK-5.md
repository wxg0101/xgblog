---
layout: post                    # 使用的布局（不需要改）
title: "DL-WEEK-5"              # 标题 
subtitle: Deep learning #副标题
date: 2022-01-12             # 时间
author: Leowxg                      # 作者
header-img: img/post-bg-hacker.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - DL
---

# WEEK 5

## Improving Deep Neural  Networks: Hyper parameter tuning,  Regularization and Optimization

### Train / Dev / Test sets

> 我们通常将样本分成训练集，验证集和测试集三部分，数据集规模相对较小，适用传统的划分比例，数据集规模较大的，验证集和测试集要小于数据总量的 20%或 10%。

### Bias /Variance

### Basic Recipe for Machine Learning

### Regularization 

### Why  regularization reduces overfitting?

### Dropout Regularization

 ### Understanding Dropout

### Other regularization methods

### Normalizing inputs

假设一个训练集有两个特征， 输入特征为 2 维，归一化需要两个步骤： 

1. 零均值 
2. 归一化方差

![image-20220113231228080](https://s2.loli.net/2022/01/13/qtWo7u3iYC6LscI.png)

第一步是零均值化

![image-20220113231246983](https://s2.loli.net/2022/01/13/urYGhH8jMikXwtm.png)

它是一个向量，𝑥等于每个训练数据 𝑥减去𝜇，意 思是移动训练集，直到它完成零均值化。

第二步是归一化方差

![image-20220113231331709](https://s2.loli.net/2022/01/13/j6hUPwuXZBVSONs.png)

![image-20220113231411049](https://s2.loli.net/2022/01/13/ycgaC2VAo5GXqWK.png)

> 所以如果输入特征处于不同范围内，可能有些特征值从 0 到 1，有些从 1 到 1000，那么 归一化特征值就非常重要了。如果特征值处于相似范围内，那么归一化就不是很重要了。执 行这类归一化并不会产生什么危害，我通常会做归一化处理，虽然我不确定它能否提高训练 或算法速度。

### Vanishing / Exploding gradients

![image-20220113230919281](https://s2.loli.net/2022/01/13/m4YRhWdFBDrz9Eu.png)

我们忽略𝑏，假设𝑏 [𝑙]=0，如果那样的话，输出：

![image-20220113230946533](https://s2.loli.net/2022/01/13/RmKTbH98VpPSx1J.png)

在深度神经网络中，激活函数将以指数级递减，虽然我只是讨论了激活函数以与𝐿相关 的指数级数增长或下降，它也适用于与层数𝐿相关的导数或梯度函数，也是呈指数级增长或 呈指数递减。



### Weight Initialization for Deep  Networks

> 针对梯度消失和梯度爆炸问题

![image-20220113212940254](https://s2.loli.net/2022/01/13/RSzV3r6CE5YjFDQ.png)

![image-20220113212953626](https://s2.loli.net/2022/01/13/iDJLmZUhEwM4tur.png)

单个神经元可能有 4 个输入特征，从𝑥1到𝑥4，经过𝑎 = 𝑔(𝑧)处理，最终得到$\hat{y}$

暂时忽略𝑏，为了预防𝑧值过大或过小，可以看到𝑛越大，希望𝑤𝑖越小，因为𝑧是𝑤𝑖𝑥𝑖的和，如果你把很多此类项相加，希望每项值更小

最合理的方法就是设置$w_i = 1/n$, n表示神经元的输入特征数

实际上，你要做的就是设置 某层权重矩阵𝑤[𝑙] = 

![image-20220113222107536](https://s2.loli.net/2022/01/13/N4wZOLqjCfosu36.png)

![image-20220113222129864](https://s2.loli.net/2022/01/13/tXUGcVTAKo85NJQ.png)

###  Numerical approximation of gradients

![image-20220113231810045](https://s2.loli.net/2022/01/13/Z1csmYRSKoy5PfE.png)

### Gradient checking

### Gradient Checking  Implementation Notes

