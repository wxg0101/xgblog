---
layout: post                    # ä½¿ç”¨çš„å¸ƒå±€ï¼ˆä¸éœ€è¦æ”¹ï¼‰
title: "DL-WEEK-5"              # æ ‡é¢˜ 
subtitle: Deep learning #å‰¯æ ‡é¢˜
date: 2022-01-12             # æ—¶é—´
author: Leowxg                      # ä½œè€…
header-img: img/post-bg-hacker.jpg    #è¿™ç¯‡æ–‡ç« æ ‡é¢˜èƒŒæ™¯å›¾ç‰‡
catalog: true                       # æ˜¯å¦å½’æ¡£
tags:                               #æ ‡ç­¾
    - DL
---

# WEEK 5

## Improving Deep Neural  Networks: Hyper parameter tuning,  Regularization and Optimization

### Train / Dev / Test sets

> æˆ‘ä»¬é€šå¸¸å°†æ ·æœ¬åˆ†æˆè®­ç»ƒé›†ï¼ŒéªŒè¯é›†å’Œæµ‹è¯•é›†ä¸‰éƒ¨åˆ†ï¼Œæ•°æ®é›†è§„æ¨¡ç›¸å¯¹è¾ƒå°ï¼Œé€‚ç”¨ä¼ ç»Ÿçš„åˆ’åˆ†æ¯”ä¾‹ï¼Œæ•°æ®é›†è§„æ¨¡è¾ƒå¤§çš„ï¼ŒéªŒè¯é›†å’Œæµ‹è¯•é›†è¦å°äºæ•°æ®æ€»é‡çš„ 20%æˆ– 10%ã€‚

### Bias /Variance

### Basic Recipe for Machine Learning

### Regularization 

### Why  regularization reduces overfitting?

### Dropout Regularization

 ### Understanding Dropout

### Other regularization methods

### Normalizing inputs

å‡è®¾ä¸€ä¸ªè®­ç»ƒé›†æœ‰ä¸¤ä¸ªç‰¹å¾ï¼Œ è¾“å…¥ç‰¹å¾ä¸º 2 ç»´ï¼Œå½’ä¸€åŒ–éœ€è¦ä¸¤ä¸ªæ­¥éª¤ï¼š 

1. é›¶å‡å€¼ 
2. å½’ä¸€åŒ–æ–¹å·®

![image-20220113231228080](https://s2.loli.net/2022/01/13/qtWo7u3iYC6LscI.png)

ç¬¬ä¸€æ­¥æ˜¯é›¶å‡å€¼åŒ–

![image-20220113231246983](https://s2.loli.net/2022/01/13/urYGhH8jMikXwtm.png)

å®ƒæ˜¯ä¸€ä¸ªå‘é‡ï¼Œğ‘¥ç­‰äºæ¯ä¸ªè®­ç»ƒæ•°æ® ğ‘¥å‡å»ğœ‡ï¼Œæ„ æ€æ˜¯ç§»åŠ¨è®­ç»ƒé›†ï¼Œç›´åˆ°å®ƒå®Œæˆé›¶å‡å€¼åŒ–ã€‚

ç¬¬äºŒæ­¥æ˜¯å½’ä¸€åŒ–æ–¹å·®

![image-20220113231331709](https://s2.loli.net/2022/01/13/j6hUPwuXZBVSONs.png)

![image-20220113231411049](https://s2.loli.net/2022/01/13/ycgaC2VAo5GXqWK.png)

> æ‰€ä»¥å¦‚æœè¾“å…¥ç‰¹å¾å¤„äºä¸åŒèŒƒå›´å†…ï¼Œå¯èƒ½æœ‰äº›ç‰¹å¾å€¼ä» 0 åˆ° 1ï¼Œæœ‰äº›ä» 1 åˆ° 1000ï¼Œé‚£ä¹ˆ å½’ä¸€åŒ–ç‰¹å¾å€¼å°±éå¸¸é‡è¦äº†ã€‚å¦‚æœç‰¹å¾å€¼å¤„äºç›¸ä¼¼èŒƒå›´å†…ï¼Œé‚£ä¹ˆå½’ä¸€åŒ–å°±ä¸æ˜¯å¾ˆé‡è¦äº†ã€‚æ‰§ è¡Œè¿™ç±»å½’ä¸€åŒ–å¹¶ä¸ä¼šäº§ç”Ÿä»€ä¹ˆå±å®³ï¼Œæˆ‘é€šå¸¸ä¼šåšå½’ä¸€åŒ–å¤„ç†ï¼Œè™½ç„¶æˆ‘ä¸ç¡®å®šå®ƒèƒ½å¦æé«˜è®­ç»ƒ æˆ–ç®—æ³•é€Ÿåº¦ã€‚

### Vanishing / Exploding gradients

![image-20220113230919281](https://s2.loli.net/2022/01/13/m4YRhWdFBDrz9Eu.png)

æˆ‘ä»¬å¿½ç•¥ğ‘ï¼Œå‡è®¾ğ‘ [ğ‘™]=0ï¼Œå¦‚æœé‚£æ ·çš„è¯ï¼Œè¾“å‡ºï¼š

![image-20220113230946533](https://s2.loli.net/2022/01/13/RmKTbH98VpPSx1J.png)

åœ¨æ·±åº¦ç¥ç»ç½‘ç»œä¸­ï¼Œæ¿€æ´»å‡½æ•°å°†ä»¥æŒ‡æ•°çº§é€’å‡ï¼Œè™½ç„¶æˆ‘åªæ˜¯è®¨è®ºäº†æ¿€æ´»å‡½æ•°ä»¥ä¸ğ¿ç›¸å…³ çš„æŒ‡æ•°çº§æ•°å¢é•¿æˆ–ä¸‹é™ï¼Œå®ƒä¹Ÿé€‚ç”¨äºä¸å±‚æ•°ğ¿ç›¸å…³çš„å¯¼æ•°æˆ–æ¢¯åº¦å‡½æ•°ï¼Œä¹Ÿæ˜¯å‘ˆæŒ‡æ•°çº§å¢é•¿æˆ– å‘ˆæŒ‡æ•°é€’å‡ã€‚



### Weight Initialization for Deep  Networks

> é’ˆå¯¹æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸é—®é¢˜

![image-20220113212940254](https://s2.loli.net/2022/01/13/RSzV3r6CE5YjFDQ.png)

![image-20220113212953626](https://s2.loli.net/2022/01/13/iDJLmZUhEwM4tur.png)

å•ä¸ªç¥ç»å…ƒå¯èƒ½æœ‰ 4 ä¸ªè¾“å…¥ç‰¹å¾ï¼Œä»ğ‘¥1åˆ°ğ‘¥4ï¼Œç»è¿‡ğ‘ = ğ‘”(ğ‘§)å¤„ç†ï¼Œæœ€ç»ˆå¾—åˆ°$\hat{y}$

æš‚æ—¶å¿½ç•¥ğ‘ï¼Œä¸ºäº†é¢„é˜²ğ‘§å€¼è¿‡å¤§æˆ–è¿‡å°ï¼Œå¯ä»¥çœ‹åˆ°ğ‘›è¶Šå¤§ï¼Œå¸Œæœ›ğ‘¤ğ‘–è¶Šå°ï¼Œå› ä¸ºğ‘§æ˜¯ğ‘¤ğ‘–ğ‘¥ğ‘–çš„å’Œï¼Œå¦‚æœä½ æŠŠå¾ˆå¤šæ­¤ç±»é¡¹ç›¸åŠ ï¼Œå¸Œæœ›æ¯é¡¹å€¼æ›´å°

æœ€åˆç†çš„æ–¹æ³•å°±æ˜¯è®¾ç½®$w_i = 1/n$, nè¡¨ç¤ºç¥ç»å…ƒçš„è¾“å…¥ç‰¹å¾æ•°

å®é™…ä¸Šï¼Œä½ è¦åšçš„å°±æ˜¯è®¾ç½® æŸå±‚æƒé‡çŸ©é˜µğ‘¤[ğ‘™] = 

![image-20220113222107536](https://s2.loli.net/2022/01/13/N4wZOLqjCfosu36.png)

![image-20220113222129864](https://s2.loli.net/2022/01/13/tXUGcVTAKo85NJQ.png)

###  Numerical approximation of gradients

![image-20220113231810045](https://s2.loli.net/2022/01/13/Z1csmYRSKoy5PfE.png)

### Gradient checking

### Gradient Checking  Implementation Notes

