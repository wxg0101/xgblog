---
layout: post                    # 使用的布局（不需要改）
title: "DL-WEEK-7"              # 标题 
subtitle: Deep learning #副标题
date: 2022-01-14             # 时间
author: Leowxg                      # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - DL
---

# Hyperparameter tuning

## Tuning process

![image-20220114172908860](https://s2.loli.net/2022/01/14/D7mtMqHG8IRpiw1.png)

在早一代的机器学习算法中， 如果你有两个超参数，这里我会称之为超参 1，超参 2，常见的做法是在网格中取样点，像 这样，然后系统的研究这些数值。这里我放置的是 5×5 的网格，实践证明，网格可以是 5×5， 也可多可少，但对于这个例子，你可以尝试这所有的 25 个点，然后选择哪个参数效果最好。 当参数的数量相对较少时，这个方法很实用。

在深度学习领域，我们常做的，我推荐你采用下面的做法，随机选择点，所以你可以选 择同等数量的点，25 个点，接着，用这些随机取的点试验超参数的效果。之所以这么 做是因为，对于你要解决的问题而言，你很难提前知道哪个超参数最重要

实践中，你搜索的可能不止三个超参数有时很难预知，哪个是最重要的超参数，对于你 的具体应用而言，随机取值而不是网格取值表明，你探究了更多重要超参数的潜在值，无论 结果是什么。

![image-20220114173303085](https://s2.loli.net/2022/01/14/fPUJesGnu4918KR.png)

比如在二维的那个例子中，你进行了取值，也许你会发现效果最好的某个点，也许这个 点周围的其他一些点效果也很好，那在接下来要做的是放大这块小区域（小蓝色方框内）

然后在其中更密集得取值或随机取值，聚集更多的资源，在这个蓝色的方格中搜索，如果你 怀疑这些超参数在这个区域的最优结果，那在整个的方格中进行粗略搜索后，你会知道接下 来应该聚焦到更小的方格中。在更小的方格中，你可以更密集得取点。所以这种从粗到细的 搜索也经常使用。

## Using an appropriate scale to  pick hyperparameters

> picking hyperparameters at random

>  appropriate scale for hyperparameters

![image-20220114174159771](https://s2.loli.net/2022/01/14/KfXTB1RbdqSCg4H.png)

用对数标尺搜索超参数的方式会更合理，因此这里不使用线性轴，分别依次取 0.0001，0.001，0.01，0.1，1，在对数轴上均匀随机取点，这样，在 0.0001 到 0.001 之间， 就会有更多的搜索资源可用，还有在 0.001 到 0.01 之间等等。

所以在 Python 中，你可以这样做，使 ```r=-4*np.random.rand()```，然后𝑎随机取值， $𝑎 = 10^𝑟$，所以，第一行可以得出𝑟 ∈ [4,0]，那么𝑎 ∈ $[10^{−4} , 10^0$ ]，所以最左边的数字是$10^{−4}$， 最右边是$10^0$。

更常见的情况是，如果你在10𝑎和10𝑏之间取值，在此例中，这是10𝑎（0.0001），你可 以通过0.0001算出𝑎的值，即-4，在右边的值是$10^𝑏$，你可以算出𝑏的值1，即 0。你要做的就 是在[𝑎, 𝑏]区间随机均匀地给𝑟取值，这个例子中𝑟 ∈ [−4,0]，然后你可以设置𝑎的值，基于随 机取样的超参数𝑎 = $10^𝑟$。

> Hypeparameters for exponentially weighted averages

![image-20220114175028884](https://s2.loli.net/2022/01/14/ade4kBvStHRxLV1.png)

所以，如果你想研究更多正式的数学证明，关于为什么我们要这样做，为什么用线性轴 取值不是个好办法，这是因为当𝛽 接近 1 时，所得结果的灵敏度会变化，即使𝛽有微小的变 化。所以𝛽 在 0.9 到 0.9005 之间取值，无关紧要，你的结果几乎不会变化。

但𝛽值如果在 0.999 到 0.9995 之间，这会对你的算法产生巨大影响，对吧？在这两种情 况下，是根据大概 10 个值取平均。但这里，它是指数的加权平均值，基于 1000 个值，现在 是 2000 个值，因为这个公式 1 1−𝛽 ，当𝛽接近 1 时，𝛽就会对细微的变化变得很敏感。所以整 个取值过程中，你需要更加密集地取值，在𝛽 接近 1 的区间内，或者说，当1 − 𝛽 接近于 0 时，这样，你就可以更加有效的分布取样点，更有效率的探究可能的结果。

## Hyperparameters  tuning in practice: Pandas vs. Caviar

 

## Normalizing activations in a  network

![image-20220114192226524](https://s2.loli.net/2022/01/14/D4TAH1htdWvIsaY.png)

当训练一个模型，比如 logistic 回归时，归一化输入特征可以加快学习 过程。你计算了平均值，从训练集中减去平均值，计算了方差，接着根据方差归一化数据集

![image-20220114192312267](https://s2.loli.net/2022/01/14/ZOoFnyVK75vNbI9.png)

<img src="https://s2.loli.net/2022/01/14/svk7STKEI6lUVG2.png" alt="image-20220114192411286" style="zoom:67%;" />

所以现在我们已把这些𝑧值标准化，化为含平均值 0 和标准单位方差，所以𝑧的每一个分 量都含有平均值 0 和方差 1，但我们不想让隐藏单元总是含有平均值 0 和方差 1，也许隐藏 单元有了不同的分布会有意义，所以我们所要做的就是计算:

![image-20220114192501362](https://s2.loli.net/2022/01/14/RDpqz8sBcK9f61A.png)

这里𝛾和𝛽是你模型的学习参数，所以我们使用梯度下降或一些其它类似梯度下降的算法， 比如 Momentum 或者 Nesterov，Adam，你会更新𝛾和𝛽，正如更新神经网络的权重一样。

如果这些成立（$𝛾 = √𝜎^2 + 𝜀$, 𝛽 = 𝜇），那么𝑧̃ (𝑖) = 𝑧 (𝑖)。

通过对𝛾和𝛽合理设定，规范化过程，即这四个等式，从根本来说，只是计算恒等函数， 通过赋予𝛾和𝛽其它值，可以使你构造含其它平均值和方差的隐藏单元值。

![image-20220114192807659](https://s2.loli.net/2022/01/14/TOjL1knoE7AbDN2.png)

Batch 归 一化的作用是它适用的归一化过程，不只是输入层，甚至同样适用于神经网络中的深度隐藏 层。你应用 Batch 归一化了一些隐藏单元值中的平均值和方差

不过训练输入和这些隐藏单 元值的一个区别是，你也许不想隐藏单元值必须是平均值 0 和方差 1。

![image-20220114203448751](https://s2.loli.net/2022/01/14/VhwJbciLsmUN27G.png)

比如，如果你有 sigmoid 激活函数，你不想让你的值总是全部集中在这里，你想使它们 有更大的方差，或不是 0 的平均值，以便更好的利用非线性的 sigmoid 函数，而不是使所有 的值都集中于这个线性版本中，这就是为什么有了𝛾和𝛽两个参数后，你可以确保所有的𝑧 (𝑖) 值可以是你想赋予的任意值，或者它的作用是保证隐藏的单元已使均值和方差标准化

## Fitting Batch Norm into  a neural network

![image-20220114204423724](https://s2.loli.net/2022/01/14/jhvl6FYxPWsIAZk.png)



你可以认为每个单元负责计算两件事。第 一，它先计算𝑧，然后应用其到激活函数中再计算𝑎，所以我可以认为，每个圆圈代表着两步 的计算过程。

同样的，对于下一层而言，那就是𝑧1 [2]和𝑎1 [2]等。所以如果你没有应用 Batch 归 一化，你会把输入𝑋拟合到第一隐藏层，然后首先计算𝑧 [1]，这是由𝑤[1]和𝑏 [1]两个参数控制 的。接着，通常而言，你会把𝑧 [1]拟合到激活函数以计算𝑎 [1]

但 Batch 归一化的做法是将𝑧 [1] 值进行 Batch 归一化，简称 BN，此过程将由𝛽 [1]和𝛾 [1]两参数控制，这一操作会给你一个新 的规范化的𝑧 [1]值（𝑧̃ [1]），然后将其输入激活函数中得到𝑎 [1]，即𝑎 [1] = 𝑔 [1] (𝑧̃ [𝑙] )。

![image-20220114205442954](https://s2.loli.net/2022/01/14/OaMKpsyYgxfFi3P.png)

Batch 归一化是发生在计算𝑧和𝑎之间的。直觉就是，与其应用没有归 一化的𝑧值，不如用归一过的𝑧̃，这是第一层（𝑧̃ [1]）。第二层同理，与其应用没有规范过的 𝑧 [2]值，不如用经过方差和均值归一后的𝑧̃ [2]。

![image-20220114205636751](https://s2.loli.net/2022/01/14/FOCLvMzwi2V7QSr.png)

参数𝑤[1]，𝑏 [1]到𝑤[𝑙]，𝑏 [𝑙]，我们将另一些 参数加入到此新网络中𝛽 [1]，𝛽 [2]，𝛾 [1]，𝛾 [2]等等。

所以现在，这是你算法的新参数，接下来你可以使用想用的任何一种优化算法，比如使 用梯度下降法来执行它。

举个例子，对于给定层，你会计算𝑑𝛽 [𝑙]，接着更新参数𝛽为𝛽 [𝑙] = 𝛽 [𝑙] − 𝛼𝑑𝛽 [𝑙]。你也可 以使用 Adam 或 RMSprop 或 Momentum，以更新参数𝛽和𝛾，并不是只应用梯度下降法。

实践中，Batch 归一化通常和训练集的 mini-batch 一起使用。

![image-20220114205921972](https://s2.loli.net/2022/01/14/VE6M1JmeYxBphbZ.png)

## Why does Batch Norm work?

归一化输入特征值𝑥，使其均值为 0，方差 1，它又是怎样 加速学习的，有一些从 0 到 1 而不是从 1 到 1000 的特征值，通过归一化所有的输入特征值 𝑥，以获得类似范围的值，可以加速学习。所以 Batch 归一化起的作用的原因，直观的一点 就是，它在做类似的工作，但不仅仅对于这里的输入值，还有隐藏单元的值

- “Covariate shift”

![image-20220114215051949](https://s2.loli.net/2022/01/14/tqXZg2J4jGWfHLE.png)

让我先遮住左边的部分，从第三隐藏层的角度来看，它得到一些值，称为𝑎1 [2]，𝑎2 [2]，𝑎3 [2]， 𝑎4 [2]，但这些值也可以是特征值𝑥1，𝑥2，𝑥3，𝑥4，第三层隐藏层的工作是找到一种方式，使 这些值映射到$\hat{y}$

现在我们把网络的左边揭开，这个网络还有参数𝑤[2]，𝑏 [2]和𝑤[1]，𝑏 [1]，如果这些参数 改变，这些𝑎 [2]的值也会改变。所以从第三层隐藏层的角度来看，这些隐藏单元的值在不断 地改变，所以它就有了“Covariate shift”的问题

![image-20220114215241116](https://s2.loli.net/2022/01/14/bQf3muRBkXHC78W.png)

Batch 归一化可以确保无论其怎样变化𝑧1 [2]，𝑧2 [2]的均值和方差保持不 变，所以即使𝑧1 [2]，𝑧2 [2]的值改变，至少他们的均值和方差也会是均值 0，方差 1，或不一定 必须是均值 0，方差 1，而是由𝛽 [2]和𝛾 [2]决定的值。如果神经网络选择的话，可强制其为均 值 0，方差 1，或其他任何均值和方差。它限制了在前层的参数更新，影响 数值分布的程度

Batch 归一化还有一个作用，它有轻微的正则化效果，Batch 归一化中非直观的一件事 是，因为在 mini-batch 上计算的均值和方差，而不是在整个数据集上，均值和方 差有一些小的噪声

![image-20220114215832268](https://s2.loli.net/2022/01/14/BfMCLDX4bmx2oql.png)

## Batch Norm at test time

Batch 归一化将你的数据以 mini-batch 的形式逐一处理，但在测试时，你可能需要对每 个样本逐一处理

![image-20220114220634691](https://s2.loli.net/2022/01/14/xk68ejEYovcLJ7w.png)

## Softmax regression

假设你不单需要识别猫，而是想识别猫，狗和小鸡，我把猫加做类 1，狗为类 2，小鸡 是类 3，如果不属于以上任何一类，就分到“其它”或者说“以上均不符合”这一类，我把它叫 做类 0

![image-20220114220840694](https://s2.loli.net/2022/01/14/EKYfG15RZNgekyp.png)

![image-20220114220859695](https://s2.loli.net/2022/01/14/TtjC8RZHSNmKwr5.png)

因此这里的𝑦^将是一个4 × 1维向量，因为它 必须输出四个数字，给你这四种概率，因为它们加起来应该等于 1

![image-20220114221014418](https://s2.loli.net/2022/01/14/KgAG1w7nHJE8Cic.png)

和往常一样，计算方法是𝑧 [𝑙] = 𝑊[𝑙]𝑎 [𝐿−1] + 𝑏 [𝑙]，算出了𝑧之后， 你需要应用 Softmax 激活函数，这个激活函数对于 Softmax 层而言有些不同

我们要计算一个临时变量，我们把它叫做 t，它等于𝑒 ^𝑧 [𝑙]，这适用于每个元素， 而这里的𝑧 [𝑙]，在我们的例子中，𝑧 [𝑙]是 4×1 的，四维向量𝑡 = 𝑒 ^𝑧 [𝑙]，这是对所有元素求幂，𝑡 也是一个 4×1 维向量，然后输出的𝑎 [𝑙]，基本上就是向量𝑡，但是会归一化，使和为 1。

![image-20220114221219076](https://s2.loli.net/2022/01/14/NsToO79Qbzj4Wpx.png)